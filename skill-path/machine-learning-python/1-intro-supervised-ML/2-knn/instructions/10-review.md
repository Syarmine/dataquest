# Review

As we suspected, our model's performance didn't improve very much. In fact, our model's accuracy dipped just a little, from 55.47% to 55.14%. If we were to randomly guess a particular data point's class, we would be correct 50% of the time since there are only two classes. So our model is only performing slightly better than a random guess right now.

However, this is not a cause for concern. In future lessons, we'll learn how we can improve our model. It's important to bear in mind that building, training, and fine-tuning machine learning models is an iterative process.

In this lesson, we learned:

- What the K-Nearest Neighbors algorithm is.
- How to implement it from scratch, both using a single feature and using multiple features.
- About feature engineering, specifically:
- Encoding our categorical variables into numerical values, and
- Normalizing our features using min-max scaling.

In the next lesson, we'll learn more about evaluating our models